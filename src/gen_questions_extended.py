"""
gen_questions_extended.py | Author : Catherine Wong

This generates program-induction style questions for the extended CLEVR templates.
The questions have n scene inputs, rather than just one; the extended CLEVR templates
also use a modified execution engine (extended_question_engine.py) to support additional primitives (such as transform or remove), and include Scene -> Scene 
tasks (e.g. "Find all the red objects", or "What if all the small blue cubes were large?").

It requires question_templates JSON files, and a set of scenes grouped by the presence of a shared, filterable object set (e.g. scenes that all have small blue cubes), generated by gen_n_inputs.py

Example usage: TODO

"""
import argparse, json, os, itertools, random, shutil
import time, copy
import re
import extended_question_engine as extended_qeng
import question_utils as qutils
from collections import defaultdict, Counter
from gen_n_inputs import UNIQUE, MULTIPLE, GROUPING_TEMPLATE_TYPES

DEFAULT_TRAIN_SCENES = "../metadata/clevr_shared_metadata/scenes/CLEVR_train_scenes_5000.json"
DEFAULT_VAL_SCENES = "../metadata/clevr_shared_metadata/scenes/CLEVR_val_scenes_5000.json"
DEFAULT_GROUPED_TRAIN_SCENES = "../metadata/clevr_shared_metadata/grouped_scenes/grouped_CLEVR_train_scenes_5000.json" # Grouped scenes generated from the train scenes.
DEFAULT_GROUPED_VAL_SCENES = "../metadata/clevr_shared_metadata/grouped_scenes/grouped_CLEVR_val_scenes_5000.json" # Grouped scenes generated from the val scenes.

DEFAULT_EXTENDED_QUESTION_METADATA = "../metadata/clevr_extended_metadata/2_metadata.json"

DEFAULT_EXTENDED_TEMPLATES_DIR = "../metadata/clevr_extended_metadata/question_templates"

GENERATE_ALL_FLAG = 'all'

# Special extended question primitives
PRIMITIVE_TRANSFORM = "transform"
PRIMITIVE_REMOVE = "remove"

# File handling.
parser = argparse.ArgumentParser()
parser.add_argument('--random_seed', default=0)

# Arguments for the scenes used to generate inputs.
parser.add_argument('--default_train_scenes', help="If provided, we use a default set of training scenes and their grouped scenes.", action='store_true')
parser.add_argument('--default_val_scenes', help="If provided, we use a default set of validation scenes and their grouped scenes.", action='store_true')
parser.add_argument('--input_scene_file', 
    help="If provided, we reference a specific set of input scenes.")
parser.add_argument('--input_grouped_scene_file', 
    help="If provided, we reference a specific set of grouped input scenes. Note that this must be generated from the same scene file as the provided input_scene_file in this case.")
    
# Which metadata file to use for the questions.
parser.add_argument('--metadata_file', default=DEFAULT_EXTENDED_QUESTION_METADATA, help="JSON file containing metadata about functions")

# Arguments used to determine which question templates to use. 
parser.add_argument('--template_dir', default=DEFAULT_EXTENDED_TEMPLATES_DIR, help="Directory containing JSON templates for questions. Note that these should only be templates for the extended template questions, not original CLEVR questions.")
parser.add_argument('--question_templates', 
                    nargs='*',
                    help='Which question templates to generate for. Use "all" for all of the templates in the directory.')
                    
# Arguments that determine parameters of the questions we generate.       
parser.add_argument('--instances_per_template', default=15, type=int,
    help="The number of times each template should be instantiated.")

# Arguments to determine where we write and store the output questions.
parser.add_argument('--output_questions_directory',
    help="Directory in which to write out the generated questions.",
    required=True)
parser.add_argument('--output_questions_prefix',
    default="CLEVR",
    help="If included, a different prefix for the output files.")

def set_random_seed(args):
    seed = args.random_seed
    print(f"Setting random seed to: {seed}")
    random.seed(seed)
    
def uniform_bernoulli_flip():
    return random.uniform(0, 1) > 0.5

def get_input_scenes_and_grouped_scenes(args):
    """
    Loads the input scenes as a provided file or from a default, along with the 
    corresponding pre-processed 'grouped scenes' file containing scenes grouped
    by a filterable object.
    Grouped scenes should have been generated by gen_n_inputs.py, and contain "unique" and "multiple" object scenes based on how many of each filterable object they contain.
    """
    if args.default_train_scenes:
        assert(not args.default_val_scenes)
        scenes_file = DEFAULT_TRAIN_SCENES
        grouped_scenes_file = DEFAULT_GROUPED_TRAIN_SCENES
    elif args.default_val_scenes:
        assert(not args.default_train_scenes)
        scenes_file = DEFAULT_VAL_SCENES
        grouped_scenes_file = DEFAULT_GROUPED_VAL_SCENES
    elif args.input_scene_file:
        assert args.input_grouped_scene_file
        scenes_file = args.input_scene_file
        grouped_scenes_file = args.input_grouped_scene_file
        
    else:
        raise RuntimeError('No input scene or grouped scene file provided.')
    
    with open(scenes_file, 'r') as f:
        scene_data = json.load(f)
        all_scenes = scene_data['scenes']
        scene_info = scene_data['info']
    print(f"Using [{len(all_scenes)}] scenes from {scenes_file}")
    
    # Read file containing input grouped scene templates.
    with open(grouped_scenes_file, 'r') as f:
        grouped_scene_data = json.load(f)
        assert grouped_scene_data['info']['split'] == scene_info['split']
        assert grouped_scene_data['info']['version'] == scene_info['version']
        grouped_scenes = grouped_scene_data['grouped_scenes']
        
        unique_object_scenes, multiple_object_scenes = grouped_scenes['unique'], grouped_scenes['multiple']
        # Check the length of the first scene group in the 'unique' set as an example.
        num_scenes_per_group = len(unique_object_scenes['0']['input_image_filenames'])
    print(f"Using scenes from {grouped_scenes_file} with {num_scenes_per_group} scenes per instance and {len(unique_object_scenes)} unique object scenes; and {len(multiple_object_scenes)} multiple_object_scenes.")
    
    return scenes_file, all_scenes, scene_info, grouped_scenes_file, grouped_scenes

def get_question_metadata(args):    
    with open(args.metadata_file, 'r') as f:
      metadata = json.load(f)
      dataset = metadata['dataset']
    functions_by_name = {}
    for f in metadata['functions']:
      functions_by_name[f['name']] = f
    metadata['_functions_by_name'] = functions_by_name
    assert PRIMITIVE_TRANSFORM in metadata['_functions_by_name']
    assert PRIMITIVE_REMOVE in metadata['_functions_by_name']
    
    return metadata

def get_question_templates(args, metadata):
    """Returns a dictionary of question templates for each file containing templates on a partiular question class: {template_filename :
        {(template_filename, original_question_idx): template}
    }"""
    generate_all = (args.question_templates == [GENERATE_ALL_FLAG])
    
    candidate_template_files = [file for file in os.listdir(args.template_dir) if file.endswith('.json')]
    
    templates = dict()
    for candidate_template_file in candidate_template_files:
        template_class_name = candidate_template_file.split('.json')[0]
        if generate_all or (template_class_name in args.question_templates):
            full_template_filepath = os.path.join(args.template_dir, candidate_template_file)
            templates[template_class_name] = dict()
            
            with open(full_template_filepath, 'r') as f:
                question_templates = json.load(f)
                for question_idx, template in enumerate(question_templates):
                    template_key = (template_class_name, question_idx)
                    templates[template_class_name][template_key] = template
                    
                print(f'Read {len(templates[template_class_name])} question templates from {full_template_filepath}.')
    return templates

def build_filter_option(grouped_input_scenes, f, constraints, group, params, original_idx, curr_node_idx, new_node_idxs):
    # Select a filter option from the grouped input scenes.
    if "filter" not in constraints:
        filter_option_idx = random.choice(list(grouped_input_scenes[group].keys()))
        input_scenes = grouped_input_scenes[group][str(filter_option_idx)]
        filter_options = input_scenes["filter_options"]
    else:
        param_to_type = {p['name'] : p['type'] for p in params}
        # Directly search for a filter that meets the required constraints
        if constraints["filter"] == "choose_exactly":
            req_types = set([param_to_type[p] for p in f['side_inputs']])
            key_options = []
            for k in grouped_input_scenes[group]:
                input_scenes = grouped_input_scenes[group][k]
                filter_options = input_scenes["filter_options"]
                filter_types = set([opt[0] for opt in filter_options])
                if req_types == filter_types:
                    key_options.append(k)
            filter_option_idx = random.choice(key_options)
            input_scenes = grouped_input_scenes[group][str(filter_option_idx)]
            filter_options = input_scenes["filter_options"]
    
    filter_program = []
    for i, (attr_type, attr_value) in enumerate(filter_options):
        # Redirect these nodes to filter from each other.
        input_idx = new_node_idxs[f['inputs'][0]] if len(filter_program) < 1 else curr_node_idx + len(filter_program) - 1
        filter_program.append({
            "type": f"filter_{attr_type.lower()}",
            "side_inputs": [attr_value],
            "inputs": [input_idx]
        })
        for p in params: 
            if p['type'] == attr_type and p['name'] in f['side_inputs']:
                p['value'] = attr_value
    # Redirect anything that filtered from this node to filter from the final filter node.
    new_node_idxs[original_idx] = curr_node_idx + len(filter_program) - 1
    return filter_program, input_scenes

def build_transform_option(f, constraints, metadata, params, original_idx, curr_node_idx, new_node_idxs):
    # Expand the transform into several params that have not yet been used.
    param_to_type = {p['name'] : p['type'] for p in params}
    transform_program = []
    while len(transform_program) < 1:
        for param in params: 
            if param['name'] in f['side_inputs']:
                if uniform_bernoulli_flip() or ("transform" in constraints and constraints["transform"] == "choose_all"):
                    used_param_vals = set([p['value'] for p in params if 'value' in p])
                    param_type = param_to_type[param['name']]
                    param_choices = set(metadata['types'][param_type]) - used_param_vals
                    param_choice = random.choice(list(param_choices))
                    param['value'] = param_choice
                    
                    # Redirect these transform nodes to take each other as the input scene
                    input_scene = new_node_idxs[f['inputs'][0]] if len(transform_program) < 1 else curr_node_idx + len(transform_program) - 1
                    selector_set = new_node_idxs[f['inputs'][1]]
                    inputs = [input_scene, selector_set]
                    
                    transform_program.append({
                        "type": f"transform_{param_type.lower()}",
                        "side_inputs": [param_choice],
                        "inputs": inputs
                    })
    
    # Redirect anything that pointed to this node to take from the final transform node
    new_node_idxs[original_idx] = len(transform_program) + curr_node_idx - 1
    return transform_program

def instantiate_question_text(template, params, constraints):
    instantiated_texts = []
    for template_text in template['text']:
        instantiated_text = template_text
        for p in params:
            if 'value' not in p:
                if p['name'] in constraints:
                     # If we failed to instantiate one of the necessary values.
                     return []
                p_value = "" if p['type'] != 'Shape' else "thing"
            else:
                p_value = p['value']
            instantiated_text = instantiated_text.replace(p['name'], p_value)
        # Remove extraneous spaces
        instantiated_text = " ".join(instantiated_text.split())
        instantiated_texts.append(instantiated_text)
    return instantiated_texts

def instantiate_param_random(param_name, metadata, params):
    for p in params:
        if p['name'] == param_name:
            param_choices = set(metadata['types'][p['type']])
            param_choice = random.choice(list(param_choices))
            p['value'] = param_choice
            return p['value']
            
def build_instantiated_program_node(f, metadata, curr_node_idx, original_idx, new_node_idxs, params):
    param_to_val = {p['name'] : p['value'] for p in params if 'value' in p}
    
    # Instantiate any unbound parameters.
    if 'side_inputs' not in f:
        side_inputs = []
    else:
        side_inputs = []
        for param_name in f['side_inputs']:
            if param_name in param_to_val:
                side_inputs.append(param_to_val[param_name])
            else:
                # Instantiate any unbound parameters.
                side_inputs.append(instantiate_param_random(param_name, metadata, params))
    
    program =  [{
        "type" : f["type"],
        "side_inputs" : side_inputs,
        "inputs" : [new_node_idxs[old_idx] for old_idx in f["inputs"]]
    }]
    new_node_idxs[original_idx] = len(program) + curr_node_idx - 1
    return program
    

def instantiate_template(
    all_input_scenes,
    grouped_input_scenes,
    template,
    metadata):
    """Instantiate template scenes with a single filter option.
    Choose from the preselected 'filter' and layer additional transformations
    on top.
    """
    program, params, input_scenes = [], template["params"], None
    
    # Build up and expand the instantiated program, changing the program node pointers as needed.
    new_node_idxs = {i : i for i in range(len(template['nodes']))}
    for original_idx, f in enumerate(template['nodes']):
        if f['type'] == 'scene':
            program.append(f)
        elif f['type'] == 'filter':
            filter_program, input_scenes = build_filter_option(grouped_input_scenes, f, template["constraints"], template['group'], params, original_idx=original_idx, curr_node_idx=len(program), new_node_idxs=new_node_idxs)
            program += filter_program
        elif f['type'] == 'transform':
            program += build_transform_option(f, template["constraints"], metadata, params, original_idx=original_idx, curr_node_idx=len(program), new_node_idxs=new_node_idxs)
        else:
            program += build_instantiated_program_node(f, metadata, curr_node_idx=len(program), original_idx=original_idx, new_node_idxs=new_node_idxs, params=params)
            
    # Build the program text
    instantiated_text = instantiate_question_text(template, params, template["constraints"])
    if len(instantiated_text) < 1:
        return [], None, None, None
    
    # Run the program on the scenes to generate the answers
    answers = []
    for input_image_index in input_scenes['input_image_indexes']:
        input_scene = all_input_scenes[int(input_image_index)]
        ans = extended_qeng.answer_question(program, 
                                            metadata,
                                            input_scene, all_outputs=False, cache_outputs=False)
    
        answers.append(ans)
    # Don't allow the answers to all be identical
    if type(answers[0]) is not dict and len(set(answers)) < 2:
        return [], None, None, None
    return instantiated_text, program, input_scenes, answers
        
def instantiate_templates_extended(all_input_scenes,
                                   grouped_input_scenes,
                                   template,
                                   metadata,
                                   max_instances,
                                   max_time=100,
                                   max_tries=1000):
    final_qs = []
    text_questions = set()
    tic = time.time()
    for _ in range(max_tries):
        t, program, input_scenes, ans = instantiate_template(all_input_scenes,
                                           grouped_input_scenes,
                                           copy.deepcopy(template),
                                           metadata) 
        if len(t) < 1: continue
        if t[0] not in text_questions:
            text_questions.update(t)
            # Fix the program as per the original code
            for f in program:
              if 'side_inputs' in f:
                f['value_inputs'] = f['side_inputs']
                del f['side_inputs']
              else:
                f['value_inputs'] = []
            
            final_qs.append({
                'question': t,
                'answers' : ans,
                'program' : program,
                'image_filenames' : input_scenes['input_image_filenames'],
                'image_indices' : input_scenes['input_image_indexes']
            })
        toc = time.time()
        # if (toc - tic) > max_time:
        #     print("Out of time!")
        #     break     
        if len(final_qs) >= max_instances:
            break
    return final_qs


def main(args):
    set_random_seed(args)
    scenes_file, all_scenes, scene_info, grouped_scenes_file, grouped_scenes = get_input_scenes_and_grouped_scenes(args)
    metadata = get_question_metadata(args)
    all_question_templates = get_question_templates(args, metadata)
    # # Load templates from disk
    # # Key is (filename, file_idx)
    # num_loaded_templates = 0
    # num_bool = 0
    # templates = {}
    # for fn in os.listdir(args.template_dir):
    #   if not fn.endswith('.json'): continue
    #   if args.question_templates is not None and os.path.basename(fn.split('.json')[0]) not in args.question_templates:
    #       continue
    #   with open(os.path.join(args.template_dir, fn), 'r') as f:
    #     base = os.path.splitext(fn)[0]
    #     for i, template in enumerate(json.load(f)):
    #         num_loaded_templates += 1
    #         key = (fn, i)
    #         templates[key] = template
    # print(f'Read {num_loaded_templates} templates from disk.')
    
    questions = []
    templates_items = list(templates.items())
    for i, ((fn, idx), template) in enumerate(templates_items):
        print(f'trying template {fn} {idx} : {i}/{len(templates_items)}') 
        instantiated_qs = instantiate_templates_extended(all_scenes,
                                       grouped_scenes,
                                       template,
                                       metadata,
                                       args.instances_per_template)
        for q in instantiated_qs:
            q['split'] = scene_info['split']
            q['template_filename'] = fn
            q['question_index'] = len(questions)
            q['template_index'] = idx
            questions.append(q)
        print(f"Generated {len(instantiated_qs)} questions for that template.")
    
    print(f"Generated {len(questions)} questions!")
    with open(args.output_questions_file, 'w') as f:
      print('Writing output to %s' % args.output_questions_file)
      json.dump({
          'info': scene_info,
          'questions': questions,
        }, f)

if __name__ == '__main__':
  args = parser.parse_args()
  main(args)    